{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "\n",
    "class DummyLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        input_ids = kwargs.get('input_ids', None)\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"Input IDs trebuie furnizate pentru predic»õii.\")\n",
    "\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        vocab_size = self.config.vocab_size\n",
    "        \n",
    "        random_logits = torch.rand((batch_size, seq_len, vocab_size), dtype=torch.float64)        \n",
    "\n",
    "        return CausalLMOutputWithPast(logits = random_logits, loss = None)\n",
    "\n",
    "config = LlamaConfig(\n",
    "    architectures=[\"LlamaForCausalLM\"],\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    hidden_act=\"silu\",\n",
    "    hidden_size=512,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=1024,\n",
    "    max_position_embeddings=256,\n",
    "    model_type=\"llama\",\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=16,\n",
    "    num_key_value_heads=8,\n",
    "    pad_token_id=0,\n",
    "    pretraining_tp=1,\n",
    "    rms_norm_eps=1e-06,\n",
    "    rope_scaling=None,\n",
    "    rope_theta=10000.0,\n",
    "    tie_word_embeddings=False,\n",
    "    torch_dtype=\"float32\",\n",
    "    transformers_version=\"4.40.1\",\n",
    "    use_cache=True,\n",
    "    vocab_size=16000\n",
    ")\n",
    "\n",
    "dummy_model = DummyLlamaForCausalLM(config)\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"babylm/babyllama-10m-2024\")\n",
    "sentence_good = \"These fathers of Linda aren't escaped from by Laurie.\"\n",
    "sentence_bad = \"These fathers of Linda aren't conferred by Laurie.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy_outputs_good = dummy_model(**inputs_good)\n",
    "    dummy_outputs_bad = dummy_model(**inputs_bad)\n",
    "    outputs_good = model(**inputs_good)\n",
    "    outputs_bad = model(**inputs_bad)\n",
    "\n",
    "\n",
    "dummy_logits_good = dummy_outputs_good.logits[:, :-1, :]\n",
    "dummy_logits_bad = dummy_outputs_bad.logits[:, :-1, :]\n",
    "logits_good = outputs_good.logits[:, :-1, :]\n",
    "logits_bad = outputs_bad.logits[:, :-1, :]\n",
    "\n",
    "target_good = inputs_good.input_ids[:, 1:]\n",
    "target_bad = inputs_bad.input_ids[:, 1:]\n",
    "\n",
    "dummy_log_probs_good = F.log_softmax(dummy_logits_good, dim=-1)\n",
    "dummy_log_probs_bad = F.log_softmax(dummy_logits_bad, dim=-1)\n",
    "log_probs_good = F.log_softmax(logits_good, dim=-1)\n",
    "log_probs_bad = F.log_softmax(logits_bad, dim=-1)\n",
    "\n",
    "dummy_sentence_log_prob_good = dummy_log_probs_good.gather(2, target_good.unsqueeze(-1)).squeeze(-1)\n",
    "dummy_sentence_log_prob_bad = dummy_log_probs_bad.gather(2, target_bad.unsqueeze(-1)).squeeze(-1)\n",
    "sentence_log_prob_good = log_probs_good.gather(2, target_good.unsqueeze(-1)).squeeze(-1)\n",
    "sentence_log_prob_bad = log_probs_bad.gather(2, target_bad.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "dummy_total_log_prob_good = dummy_sentence_log_prob_good.sum(dim=-1).item()\n",
    "dummy_total_log_prob_bad = dummy_sentence_log_prob_bad.sum(dim=-1).item()\n",
    "total_log_prob_good = sentence_log_prob_good.sum(dim=-1).item()\n",
    "total_log_prob_bad = sentence_log_prob_bad.sum(dim=-1).item()\n",
    "\n",
    "print(f\"Log-probability of sentence_good with dummy: {dummy_total_log_prob_good}\")\n",
    "print(f\"Log-probability of sentence_bad with dummy: {dummy_total_log_prob_bad}\")\n",
    "print(f\"Log-probability of sentence_good: {total_log_prob_good}\")\n",
    "print(f\"Log-probability of sentence_bad: {total_log_prob_bad}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
