{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "\n",
    "config = LlamaConfig(\n",
    "    architectures=[\"LlamaForCausalLM\"],\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    hidden_act=\"silu\",\n",
    "    hidden_size=512,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=1024,\n",
    "    max_position_embeddings=256,\n",
    "    model_type=\"llama\",\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=16,\n",
    "    num_key_value_heads=8,\n",
    "    pad_token_id=0,\n",
    "    pretraining_tp=1,\n",
    "    rms_norm_eps=1e-06,\n",
    "    rope_scaling=None,\n",
    "    rope_theta=10000.0,\n",
    "    tie_word_embeddings=False,\n",
    "    torch_dtype=\"float32\",\n",
    "    transformers_version=\"4.40.1\",\n",
    "    use_cache=True,\n",
    "    vocab_size=16000\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"babylm/babyllama-10m-2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "training_folder_path = '/kaggle/input/strict-small/train_10M' \n",
    "dev_folder_path = '/kaggle/input/development-set/dev'\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    encoding = tokenizer(text, padding=True, truncation=False)\n",
    "    num_tokens = len(encoding['input_ids']) \n",
    "    print(f\"Fi»ôierul {file_path} are {num_tokens} tokeni.\")\n",
    "    return encoding\n",
    "\n",
    "def create_dataset(tokenized_files):\n",
    "    data = [] \n",
    "    for tokenized_file in tokenized_files:\n",
    "        input_ids = tokenized_file['input_ids'] \n",
    "        attention_mask = tokenized_file['attention_mask']\n",
    "        data.append({'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': input_ids})\n",
    "\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "training_files = [os.path.join(training_folder_path, f) for f in os.listdir(training_folder_path)]\n",
    "dev_files = [os.path.join(dev_folder_path, f) for f in os.listdir(dev_folder_path)]\n",
    "\n",
    "training_tokenized_files = []\n",
    "for filename in training_files:\n",
    "    encoding = tokenize_file(filename)\n",
    "    training_tokenized_files.append(encoding)\n",
    "\n",
    "dev_tokenized_files = []\n",
    "for filename in dev_files:\n",
    "    encoding = tokenize_file(filename)\n",
    "    dev_tokenized_files.append(encoding)\n",
    "\n",
    "tokenized_training_set = create_dataset(training_tokenized_files)\n",
    "tokenized_dev_set = create_dataset(dev_tokenized_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(input_ids, max_length=256):\n",
    "    return [\n",
    "        input_ids[i:i + max_length] + [0] * max(0, max_length - len(input_ids[i:i + max_length]))\n",
    "        for i in range(0, len(input_ids), max_length)]\n",
    "\n",
    "def split_dataset(dataset, max_length=256):\n",
    "    split_data = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "    \n",
    "    for example in dataset:\n",
    "        input_ids = example['input_ids']\n",
    "        attention_mask = example['attention_mask']\n",
    "        labels = example['labels']\n",
    "        \n",
    "        split_input_ids = split_sequence(input_ids, max_length)\n",
    "        split_attention_mask = split_sequence(attention_mask, max_length)\n",
    "        split_labels = split_sequence(labels, max_length)\n",
    "        \n",
    "        for i in range(len(split_input_ids)):\n",
    "            split_data['input_ids'].append(split_input_ids[i])\n",
    "            split_data['attention_mask'].append(split_attention_mask[i])\n",
    "            split_data['labels'].append(split_labels[i])\n",
    "\n",
    "    return Dataset.from_dict(split_data)\n",
    "\n",
    "\n",
    "tokenized_training_set = split_dataset(tokenized_training_set)\n",
    "print(tokenized_training_set)\n",
    "tokenized_dev_set = split_dataset(tokenized_dev_set)\n",
    "print(tokenized_dev_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "file_path = \"/kaggle/input/training-args/training_args.bin\"\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "loaded_args = torch.load(file_path).to_dict()\n",
    "training_args = TrainingArguments(**loaded_args)\n",
    "training_args.report_to = [] \n",
    "training_args.do_train = True\n",
    "print(training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row(index, row):\n",
    "    has_none = any(token is None for token in row['input_ids']) or \\\n",
    "               any(token is None for token in row['attention_mask'])\n",
    "    return has_none\n",
    "    \n",
    "def fix_none_values(example, idx):\n",
    "    if idx in problematic_indices:\n",
    "        for j in range(len(example['input_ids'])):\n",
    "            if example['input_ids'][j] is None:\n",
    "                example['input_ids'][j] = 0\n",
    "                example['attention_mask'][j] = 0\n",
    "                example['labels'][j] = 0\n",
    "    return example\n",
    "    \n",
    "problematic_indices = [index for index, row in enumerate(tokenized_dev_set) if check_row(index, row)]\n",
    "\n",
    "print(problematic_indices)\n",
    "\n",
    "tokenized_dev_set = tokenized_dev_set.map(fix_none_values, with_indices=True)\n",
    "\n",
    "problematic_indices = [index for index, row in enumerate(tokenized_dev_set) if check_row(index, row)]\n",
    "print(problematic_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_set,\n",
    "    eval_dataset=tokenized_dev_set,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
